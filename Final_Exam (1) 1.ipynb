{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics.classification import MultilabelPrecision, MultilabelRecall, MultilabelF1Score\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "\n",
    "\n",
    "splits = {'java_train': 'data/java_train-00000-of-00001.parquet', 'java_test': 'data/java_test-00000-of-00001.parquet', 'python_train': 'data/python_train-00000-of-00001.parquet', 'python_test': 'data/python_test-00000-of-00001.parquet', 'pharo_train': 'data/pharo_train-00000-of-00001.parquet', 'pharo_test': 'data/pharo_test-00000-of-00001.parquet'}\n",
    "\n",
    "java_train = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"java_train\"])\n",
    "java_test = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"java_test\"])\n",
    "\n",
    "python_train = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"python_train\"])\n",
    "python_test = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"python_test\"])\n",
    "\n",
    "pharo_train = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"pharo_train\"])\n",
    "pharo_test = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"pharo_test\"])\n",
    "\n",
    "# Split Java dataset\n",
    "java_train_data, java_val_data = train_test_split(java_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split Python dataset\n",
    "python_train_data, python_val_data = train_test_split(python_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split Pharo dataset\n",
    "pharo_train_data, pharo_val_data = train_test_split(pharo_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Java train size: {len(java_train_data)}, Java val size: {len(java_val_data)}\")\n",
    "print(f\"Python train size: {len(python_train_data)}, Python val size: {len(python_val_data)}\")\n",
    "print(f\"Pharo train size: {len(pharo_train_data)}, Pharo val size: {len(pharo_val_data)}\")\n",
    "\n",
    "#print(java_train.iloc[0, :])\n",
    "#print(python_train.iloc[0, :])\n",
    "#print(pharo_train.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JavaCommentDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.comments = dataframe['combo'].tolist()\n",
    "        self.labels = dataframe['labels'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the text\n",
    "        text = self.comments[idx]\n",
    "        tokens = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Process labels\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        \n",
    "        # Reshape input for CNN\n",
    "        input_ids = tokens['input_ids'].squeeze(0)\n",
    "        \n",
    "        # Reshape embeddings to match CNN input format [batch_size, channels, sequence_length, embedding_dim]\n",
    "        cnn_input = input_ids.unsqueeze(0)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': cnn_input,\n",
    "            'labels': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "max_len = 512 \n",
    "\n",
    "# Prepare Dataset\n",
    "train_dataset = JavaCommentDataset(java_train_data, tokenizer, max_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = JavaCommentDataset(java_val_data, tokenizer, max_len)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_dataset = JavaCommentDataset(java_test, tokenizer, max_len)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(PyTorchCNN, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "        # CNN layers definition\n",
    "        self.cnn_layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 3, kernel_size=(5, embed_dim)),  # Example embedding size\n",
    "            torch.nn.BatchNorm2d(3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=(2, 1)), \n",
    "            \n",
    "            torch.nn.Conv2d(3, 16, kernel_size=(3, 1)),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=(2, 1)), \n",
    "            \n",
    "            torch.nn.Conv2d(16, 32, kernel_size=(3, 1)),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        )\n",
    "        # Dynamically calculate flattened size\n",
    "        self.flattened_size = self._get_flattened_size(embed_dim)\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.flattened_size, 512),\n",
    "            torch.nn.BatchNorm1d(512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, num_classes)  # Output layer\n",
    "        )\n",
    "    def _get_flattened_size(self, embed_dim):\n",
    "        \"\"\"\n",
    "        Computes the size of the flattened output after the CNN layers.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Create a dummy input of shape [batch_size=1, channels=1, sequence_length, embed_dim]\n",
    "            dummy_input = torch.zeros(1, 1, 512, embed_dim)\n",
    "            cnn_out = self.cnn_layers(dummy_input)\n",
    "            return cnn_out.numel()\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.embedding(input_ids)  # Shape: [batch_size, sequence_length, embed_dim]\n",
    "\n",
    "        #Pass through CNN layers\n",
    "        cnn_out = self.cnn_layers(embeddings) \n",
    "\n",
    "        cnn_out = torch.flatten(cnn_out, 1) \n",
    "\n",
    "        output = self.fc_layers(cnn_out) \n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate, num_classes=7):\n",
    "        super().__init__()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.num_classes = num_classes\n",
    "        # Metrics\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multilabel\", num_labels=7)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multilabel\", num_labels=7)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multilabel\", num_labels=7)\n",
    "\n",
    "        # Initialize class-wise accuracy tracking\n",
    "        self.class_wise_train_acc = {i: torchmetrics.Accuracy(task=\"multilabel\", num_labels=7) for i in range(num_classes)}\n",
    "        self.class_wise_val_acc = {i: torchmetrics.Accuracy(task=\"multilabel\", num_labels=7) for i in range(num_classes)}\n",
    "        self.class_wise_test_acc = {i: torchmetrics.Accuracy(task=\"multilabel\", num_labels=7) for i in range(num_classes)}\n",
    "        # Precision Metrics\n",
    "        self.train_precision = MultilabelPrecision(num_labels=num_classes, average=\"none\")\n",
    "        self.val_precision = MultilabelPrecision(num_labels=num_classes, average=\"none\")\n",
    "        self.test_precision = MultilabelPrecision(num_labels=num_classes, average=\"none\")\n",
    "        # Recall Metrics\n",
    "        self.train_recall = MultilabelRecall(num_labels=num_classes, average=\"none\")\n",
    "        self.val_recall = MultilabelRecall(num_labels=num_classes, average=\"none\")\n",
    "        self.test_recall = MultilabelRecall(num_labels=num_classes, average=\"none\")\n",
    "\n",
    "        # F1 Metrics\n",
    "        self.train_f1 = MultilabelF1Score(num_labels=num_classes, average=\"none\")\n",
    "        self.val_f1 = MultilabelF1Score(num_labels=num_classes, average=\"none\")\n",
    "        self.test_f1 = MultilabelF1Score(num_labels=num_classes, average=\"none\")\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _shared_step(self, batch):\n",
    "        input_ids = batch['input_ids']  # Tokenized input\n",
    "        true_labels = batch['labels']  # Multi-hot encoded labels\n",
    "\n",
    "\n",
    "        if true_labels.ndim > 1:\n",
    "            true_labels = true_labels.argmax(dim=-1)\n",
    "\n",
    "        logits = self.model(input_ids)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = F.cross_entropy(logits, true_labels)\n",
    "        \n",
    "        # Compute the predicted labels by applying a threshold\n",
    "        probabilities = F.softmax(logits, dim=-1) \n",
    "\n",
    "        predicted_labels_idx = torch.argmax(probabilities, dim=-1)\n",
    "        \n",
    "        batch_size = predicted_labels_idx.size(0)\n",
    "        num_classes = probabilities.size(1)  # Number of classes\n",
    "        one_hot_predictions = torch.zeros(batch_size, num_classes, device=logits.device)\n",
    "        one_hot_predictions.scatter_(1, predicted_labels_idx.unsqueeze(1), 1)\n",
    "\n",
    "        print(\"Here:\",one_hot_predictions[1])\n",
    "        return loss, true_labels, one_hot_predictions\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.train_acc(predicted_labels, true_labels)\n",
    "        self.log(\"train_acc\", self.train_acc, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        # Class-wise accuracy logging\n",
    "        class_accuracies = self._calculate_class_accuracy(predicted_labels, true_labels)\n",
    "\n",
    "        precision_values = self.train_precision(predicted_labels, true_labels)\n",
    "        for i, precision in enumerate(precision_values):\n",
    "            self.log(f\"train_precision_class_{i}\", precision, prog_bar=True)\n",
    "\n",
    "        # Recall Logging\n",
    "        recall_values = self.train_recall(predicted_labels, true_labels)\n",
    "        for i, recall in enumerate(recall_values):\n",
    "            self.log(f\"train_recall_class_{i}\", recall, prog_bar=True)\n",
    "\n",
    "        # F1 Logging\n",
    "        f1_values = self.train_f1(predicted_labels, true_labels)\n",
    "        for i, f1 in enumerate(f1_values):\n",
    "            self.log(f\"train_f1_class_{i}\", f1, prog_bar=True)\n",
    "\n",
    "        # for class_idx, accuracy in class_accuracies.items():\n",
    "        #     self.log(f\"class_{class_idx}_train_acc\", accuracy, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.val_acc(predicted_labels, true_labels)\n",
    "        self.log(\"val_acc\", self.val_acc, prog_bar=True)\n",
    "        # Class-wise accuracy logging\n",
    "        class_accuracies = self._calculate_class_accuracy(predicted_labels, true_labels)\n",
    "\n",
    "        # for class_idx, accuracy in class_accuracies.items():\n",
    "        #     self.log(f\"class_{class_idx}_val_acc\", accuracy, prog_bar=True)\n",
    "\n",
    "        # Precision\n",
    "        precision_values = self.val_precision(predicted_labels, true_labels)\n",
    "        for i, precision in enumerate(precision_values):\n",
    "            self.log(f\"val_precision_class_{i}\", precision, prog_bar=True)\n",
    "\n",
    "        # Recall Logging\n",
    "        recall_values = self.val_recall(predicted_labels, true_labels)\n",
    "        for i, recall in enumerate(recall_values):\n",
    "            self.log(f\"val_recall_class_{i}\", recall, prog_bar=True)\n",
    "\n",
    "        # F1 Logging\n",
    "        f1_values = self.val_f1(predicted_labels, true_labels)\n",
    "        for i, f1 in enumerate(f1_values):\n",
    "            self.log(f\"val_f1_class_{i}\", f1, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "        self.test_acc(predicted_labels, true_labels)\n",
    "        self.log(\"test_acc\", self.test_acc)\n",
    "\n",
    "        class_accuracies = self._calculate_class_accuracy(predicted_labels, true_labels)\n",
    "        \n",
    "        # for class_idx, accuracy in class_accuracies.items():\n",
    "        #     self.log(f\"class_{class_idx}_test_acc\", accuracy)\n",
    "        # Precision\n",
    "        precision_values = self.test_precision(predicted_labels, true_labels)\n",
    "        for i, precision in enumerate(precision_values):\n",
    "            self.log(f\"test_precision_class_{i}\", precision)\n",
    "\n",
    "        # Recall Logging\n",
    "        recall_values = self.test_recall(predicted_labels, true_labels)\n",
    "        for i, recall in enumerate(recall_values):\n",
    "            self.log(f\"test_recall_class_{i}\", recall)\n",
    "\n",
    "        # F1 Logging\n",
    "        f1_values = self.test_f1(predicted_labels, true_labels)\n",
    "        for i, f1 in enumerate(f1_values):\n",
    "            self.log(f\"test_f1_class_{i}\", f1)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def _calculate_class_accuracy(self, predicted_labels, true_labels):\n",
    "        class_accuracies = {}\n",
    "\n",
    "        # Compute correct predictions per class\n",
    "        correct_per_class = (predicted_labels * true_labels).sum(dim=0)  # Element-wise AND followed by sum across batch\n",
    "        total_per_class = true_labels.sum(dim=0)  # Total true instances per class\n",
    "\n",
    "        # Calculate accuracy for each class, avoiding division by zero\n",
    "        for i in range(self.num_classes):\n",
    "            correct = correct_per_class[i].item()\n",
    "            total = total_per_class[i].item()\n",
    "\n",
    "            if total > 0:\n",
    "                class_accuracies[i] = correct / total\n",
    "            else:\n",
    "                class_accuracies[i] = 0.0 \n",
    "\n",
    "        return class_accuracies\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(123)\n",
    "vocab_size = 30522  #For BERT tokenizer\n",
    "embed_dim = 768     #embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model = PyTorchCNN(vocab_size=vocab_size, embed_dim=embed_dim,num_classes=7)  \n",
    "lightning_model = LightningModel(model=pytorch_model, learning_rate=0.01,num_classes=7 )\n",
    "\n",
    "# Setup PyTorch Lightning trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator=\"gpu\",  # Change to \"gpu\" if you want to use a GPU\n",
    "    devices=1,  # Number of devices (1 for single device, or change to \"auto\" to use all available GPUs)\n",
    "    logger=CSVLogger(save_dir=\"logs/\", name=\"my-model\"),\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(lightning_model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model=lightning_model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Modify Dataset Class for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PythonCommentDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len: int = 128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the comment sentence and labels\n",
    "        comment = str(self.data.iloc[idx]['comment_sentence'])\n",
    "        labels = self.data.iloc[idx]['labels']\n",
    "        \n",
    "        # Tokenize the comment text using the tokenizer\n",
    "        encoding = self.tokenizer(\n",
    "            comment,\n",
    "            add_special_tokens=True,  \n",
    "            max_length=self.max_len,  \n",
    "            padding='max_length',  \n",
    "            truncation=True,  \n",
    "            return_tensors='pt',  \n",
    "        )\n",
    "\n",
    "        # Extract input_ids and attention_mask from the encoding\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        # Convert the labels to a tensor\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        # Return a dictionary with inputs and labels\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels_tensor\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Load Python Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Python dataset into training and validation\n",
    "python_train_data, python_val_data = train_test_split(python_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# re-set tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "max_len = 512 \n",
    "\n",
    "# Prepare Dataset\n",
    "python_train_dataset = PythonCommentDataset(python_train_data, tokenizer, max_len)\n",
    "python_val_dataset = PythonCommentDataset(python_val_data, tokenizer, max_len)\n",
    "python_test_dataset = PythonCommentDataset(python_test, tokenizer, max_len)\n",
    "\n",
    "# Dataloaders\n",
    "python_train_loader = DataLoader(python_train_dataset, batch_size=32, shuffle=True)\n",
    "python_val_loader = DataLoader(python_val_dataset, batch_size=32, shuffle=False)\n",
    "python_test_loader = DataLoader(python_test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Update Models for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model for Python comments\n",
    "pytorch_python_model = PyTorchCNN(vocab_size=vocab_size, embed_dim=embed_dim, num_classes=7)\n",
    "lightning_python_model = LightningModel(model=pytorch_python_model, learning_rate=0.01, num_classes=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Train model for Python comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup PyTorch Lightning trainer for CPU\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator=\"cpu\",  # Use CPU instead of GPU\n",
    "    devices=1,  \n",
    "    logger=CSVLogger(save_dir=\"logs/\", name=\"my-model\"),\n",
    "    deterministic=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model on Python dataset\n",
    "trainer.fit(lightning_python_model, train_dataloaders=python_train_loader, val_dataloaders=python_val_loader)\n",
    "\n",
    "# Test the model on Python test dataset\n",
    "trainer.test(model=lightning_python_model, dataloaders=python_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pharo Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Setup Dataset for Pharo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PharoCommentDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.comments = dataframe['combo'].tolist()\n",
    "        self.labels = dataframe['labels'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the text\n",
    "        text = self.comments[idx]\n",
    "        tokens = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Process labels\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        \n",
    "        # Reshape input for CNN\n",
    "        input_ids = tokens['input_ids'].squeeze(0)\n",
    "        \n",
    "        # Reshape embeddings to match CNN input format [batch_size, channels, sequence_length, embedding_dim]\n",
    "        cnn_input = input_ids.unsqueeze(0)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': cnn_input,\n",
    "            'labels': label\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Reset model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "max_len = 512 \n",
    "\n",
    "# Prepare Dataset\n",
    "train_dataset = PharoCommentDataset(pharo_train_data, tokenizer, max_len)\n",
    "test_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = PharoCommentDataset(pharo_val_data, tokenizer, max_len)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_dataset = PharoCommentDataset(pharo_test, tokenizer, max_len)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Implement models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model for Python comments\n",
    "pytorch_pharo_model = PyTorchCNN(vocab_size=vocab_size, embed_dim=embed_dim, num_classes=7)\n",
    "lightning_pharo_model = LightningModel(model=pytorch_pharo_model, learning_rate=0.01, num_classes=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Set for Pharo comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup PyTorch Lightning trainer for GPU\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,  \n",
    "    logger=CSVLogger(save_dir=\"logs/\", name=\"my-model\"),\n",
    "    deterministic=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the pharo dataset\n",
    "trainer.fit(lightning_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "# Test the model on Pharo test dataset\n",
    "trainer.test(model=lightning_model, dataloaders=test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
